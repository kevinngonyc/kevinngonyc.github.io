<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_9fg3y5d606jq-0>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-0}ol.lst-kix_9fg3y5d606jq-4.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-4 0}.lst-kix_t0atw2pi7a8g-3>li:before{content:"\0025cf  "}.lst-kix_9fg3y5d606jq-5>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-5}.lst-kix_t0atw2pi7a8g-2>li:before{content:"\0025a0  "}.lst-kix_9fg3y5d606jq-6>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-6}.lst-kix_t0atw2pi7a8g-0>li:before{content:"\0025cf  "}ul.lst-kix_t0atw2pi7a8g-0{list-style-type:none}.lst-kix_t0atw2pi7a8g-1>li:before{content:"\0025cb  "}.lst-kix_4y6eds45exiy-8>li:before{content:"\0025a0  "}ol.lst-kix_9fg3y5d606jq-1.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-1 0}ul.lst-kix_t0atw2pi7a8g-2{list-style-type:none}.lst-kix_4y6eds45exiy-7>li:before{content:"\0025cb  "}ul.lst-kix_t0atw2pi7a8g-1{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-4{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-3{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-6{list-style-type:none}ul.lst-kix_4y6eds45exiy-4{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-5{list-style-type:none}ul.lst-kix_4y6eds45exiy-5{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-8{list-style-type:none}ul.lst-kix_4y6eds45exiy-2{list-style-type:none}ul.lst-kix_t0atw2pi7a8g-7{list-style-type:none}ul.lst-kix_4y6eds45exiy-3{list-style-type:none}.lst-kix_9fg3y5d606jq-7>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-7}ul.lst-kix_4y6eds45exiy-8{list-style-type:none}.lst-kix_9fg3y5d606jq-1>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-1}ul.lst-kix_4y6eds45exiy-6{list-style-type:none}ul.lst-kix_4y6eds45exiy-7{list-style-type:none}ul.lst-kix_4y6eds45exiy-0{list-style-type:none}ul.lst-kix_4y6eds45exiy-1{list-style-type:none}ol.lst-kix_9fg3y5d606jq-8.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-8 0}ol.lst-kix_9fg3y5d606jq-5.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-5 0}ol.lst-kix_9fg3y5d606jq-2.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-2 0}.lst-kix_9fg3y5d606jq-4>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-4}.lst-kix_9fg3y5d606jq-5>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-5,lower-roman) ". "}.lst-kix_9fg3y5d606jq-6>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-6,decimal) ". "}.lst-kix_9fg3y5d606jq-3>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-3,decimal) ". "}.lst-kix_9fg3y5d606jq-7>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-7,lower-latin) ". "}.lst-kix_9fg3y5d606jq-3>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-3}.lst-kix_9fg3y5d606jq-4>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-4,lower-latin) ". "}.lst-kix_9fg3y5d606jq-8>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-8}ol.lst-kix_9fg3y5d606jq-6.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-6 0}.lst-kix_9fg3y5d606jq-2>li{counter-increment:lst-ctn-kix_9fg3y5d606jq-2}.lst-kix_9fg3y5d606jq-8>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-8,lower-roman) ". "}.lst-kix_4y6eds45exiy-5>li:before{content:"\0025a0  "}ol.lst-kix_9fg3y5d606jq-3.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-3 0}.lst-kix_4y6eds45exiy-4>li:before{content:"\0025cb  "}.lst-kix_4y6eds45exiy-6>li:before{content:"\0025cf  "}ol.lst-kix_9fg3y5d606jq-4{list-style-type:none}ol.lst-kix_9fg3y5d606jq-3{list-style-type:none}ol.lst-kix_9fg3y5d606jq-6{list-style-type:none}.lst-kix_4y6eds45exiy-2>li:before{content:"\0025a0  "}ol.lst-kix_9fg3y5d606jq-5{list-style-type:none}.lst-kix_4y6eds45exiy-3>li:before{content:"\0025cf  "}ol.lst-kix_9fg3y5d606jq-8{list-style-type:none}ol.lst-kix_9fg3y5d606jq-7{list-style-type:none}.lst-kix_t0atw2pi7a8g-4>li:before{content:"\0025cb  "}.lst-kix_t0atw2pi7a8g-5>li:before{content:"\0025a0  "}ol.lst-kix_9fg3y5d606jq-0.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-0 0}.lst-kix_t0atw2pi7a8g-6>li:before{content:"\0025cf  "}ol.lst-kix_9fg3y5d606jq-0{list-style-type:none}ol.lst-kix_9fg3y5d606jq-2{list-style-type:none}ol.lst-kix_9fg3y5d606jq-7.start{counter-reset:lst-ctn-kix_9fg3y5d606jq-7 0}.lst-kix_4y6eds45exiy-1>li:before{content:"\0025cb  "}ol.lst-kix_9fg3y5d606jq-1{list-style-type:none}.lst-kix_t0atw2pi7a8g-7>li:before{content:"\0025cb  "}.lst-kix_t0atw2pi7a8g-8>li:before{content:"\0025a0  "}.lst-kix_9fg3y5d606jq-1>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-1,lower-latin) ". "}.lst-kix_9fg3y5d606jq-2>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-2,lower-roman) ". "}.lst-kix_4y6eds45exiy-0>li:before{content:"\0025cf  "}.lst-kix_9fg3y5d606jq-0>li:before{content:"" counter(lst-ctn-kix_9fg3y5d606jq-0,decimal) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c13{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:234pt;border-top-color:#000000;border-bottom-style:solid}.c30{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{padding-top:0pt;padding-bottom:10pt;line-height:2.0;orphans:2;widows:2;text-align:justify;height:11pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c15{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c27{padding-top:16pt;padding-bottom:10pt;line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c2{padding-top:16pt;padding-bottom:4pt;line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c9{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c29{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c20{margin-left:36pt;padding-top:0pt;padding-bottom:10pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c17{padding-top:18pt;padding-bottom:10pt;line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c5{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c25{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c23{padding-top:0pt;padding-bottom:10pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:justify}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c22{padding-top:10pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c21{padding-top:0pt;padding-bottom:10pt;line-height:2.0;orphans:2;widows:2;text-align:justify}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c43{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c39{padding-top:14pt;padding-bottom:4pt;line-height:2.0;page-break-after:avoid;text-align:left}.c24{font-weight:400;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c37{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c41{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c44{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c36{padding-top:4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c38{border-spacing:0;border-collapse:collapse;margin-right:auto}.c31{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c14{color:inherit;text-decoration:inherit}.c18{color:#000000;text-decoration:none}.c10{font-weight:400;font-family:"Times New Roman"}.c4{font-weight:700;font-family:"Times New Roman"}.c35{orphans:2;widows:2}.c33{width:33%;height:1px}.c46{color:#000000}.c16{vertical-align:sub}.c45{page-break-after:avoid}.c26{font-size:11pt}.c28{font-size:18pt}.c34{font-style:italic}.c19{height:0pt}.c42{margin-left:108pt}.c40{height:65pt}.c12{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c31"><p class="c21 c42"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 334.50px; height: 162.43px;"><img alt="" src="images/image6.png" style="width: 334.50px; height: 162.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c23 c45 title" id="h.uvyqo9ygh7rn"><span class="c10 c28">Insights - Knowledge Graph Final Report (Spring 2019)<br></span><span class="c0">Contributors: Jenny Chen, Kevin Ngo, Oscar So, Tushar Khan, Ziwei Gu</span></p><p class="c32 c12"><span class="c7"></span></p><hr><p class="c32 c12"><span class="c7"></span></p><p class="c32 c12"><span class="c7"></span></p><p class="c12 c32"><span class="c0"></span></p><p class="c35 c36"><span class="c4"><a class="c14" href="#h.nvddsxpc0cf">1 Introduction</a></span><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c4"><a class="c14" href="#h.nvddsxpc0cf">2</a></span></p><p class="c15"><span class="c0"><a class="c14" href="#h.q34efqshbxtt">1.1 Overview</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a class="c14" href="#h.q34efqshbxtt">2</a></span></p><p class="c15"><span class="c0"><a class="c14" href="#h.ihu99bdtzelj">1.2 Background</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a class="c14" href="#h.ihu99bdtzelj">2</a></span></p><p class="c35 c41"><span class="c11 c4"><a class="c14" href="#h.o1f557otys13">2 Methodology</a></span><span class="c11 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c4">3</span></p><p class="c15"><span class="c0"><a class="c14" href="#h.phm8ne2n9gdx">2.1 Overall Architecture</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a class="c14" href="#h.phm8ne2n9gdx">3</a></span></p><p class="c15"><span class="c0"><a class="c14" href="#h.4yq64s75g063">2.2 Entity Mapping</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c10">4</span></p><p class="c15"><span class="c0"><a class="c14" href="#h.biuqybju8g31">2.3 Coreference Resolution</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a class="c14" href="#h.biuqybju8g31">4</a></span></p><p class="c15"><span class="c0"><a class="c14" href="#h.obzq5psetisi">2.4 Triple Extraction</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c10">5</span></p><p class="c43 c35"><span class="c0"><a class="c14" href="#h.4abh9ljy07t7">2.4.1 Triple Extraction using an existing open IE system (AllenNLP)</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0"><a class="c14" href="#h.4abh9ljy07t7">5</a></span></p><p class="c35 c43"><span class="c0"><a class="c14" href="#h.4abh9ljy07t7">2.4.2 Triple Extraction using a sequence-to-sequence model</a></span><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6</span></p><p class="c35 c44"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp;2.5 Triple Integration</span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c10">9</span></p><p class="c15"><span class="c0"><a class="c14" href="#h.6mvw026lfdq1">2.6 Predicate Mapping</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c10">10</span></p><p class="c41 c35"><span class="c11 c4"><a class="c14" href="#h.623k4w2gkt4r">3</a></span><span class="c11 c4">&nbsp;Results &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</span></p><p class="c41 c35"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c10">3.1 Tuples</span><span class="c11 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c18 c10">11</span></p><p class="c15"><span class="c0"><a class="c14" href="#h.e9eeo7m5rjol">3.2 Graph Visualization</a></span><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c10 c18">12</span></p><p class="c22"><span class="c11 c4"><a class="c14" href="#h.x8r1cwvt4dpx">4 Future Goals</a></span><span class="c11 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c11 c4"><a class="c14" href="#h.x8r1cwvt4dpx">1</a></span><span class="c4">3</span></p><h2 class="c17" id="h.nvddsxpc0cf"><span class="c11 c4">1 Introduction</span></h2><h3 class="c27" id="h.q34efqshbxtt"><span class="c9">1.1 Overview</span></h3><p class="c21"><span class="c10">This project</span><sup class="c10"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c0">&nbsp;seeks to create structured representation of knowledge based on unstructured text. The end goal of the project is an end-to-end system that generates a knowledge graph from raw text, with the potential to assist users in understanding a source article and facilitate question-answering system (by enhancing searchability over entities in an article). Our resulting knowledge graphs are also natural inputs to Graph Neural Networks, a family of ML models that take graphs as inputs. </span></p><p class="c21"><span class="c0">Our system yielded substantive results, generating a set of well-formed relation tuples from any input paragraphs. We then convert the tuples to a network graph to aid visualization.</span></p><h3 class="c27" id="h.ihu99bdtzelj"><span class="c9">1.2 Background</span></h3><p class="c1"><span class="c10">A knowledge graph (KG) is a graph-structured knowledge base that stores knowledge in the form of the relation between entities (e.g. (LeBron James, plays for, Los Angeles Lakers)). </span><span class="c0">Knowledge Graphs play a critical role in many modern applications, such as question answering and query expansion. Constructing a knowledge graph from unstructured text, however, is a challenging task due to the variety of open relations and the massive ambiguity in text data. Therefore, we focus on exactly these two challenges - extracting open relation triples from raw text and reducing the ambiguities within them.</span></p><h2 class="c17" id="h.o1f557otys13"><span class="c11 c4">2 Methodology</span></h2><h3 class="c2" id="h.phm8ne2n9gdx"><span class="c9">2.1 Overall Architecture</span></h3><p class="c5"><span class="c0">In this section, the overall architecture of our system is described. Our model is designed to take unstructured text as input and produce a knowledge graph as output. As shown in Figure 1, it has the following five components: Triple Extraction, Entity Mapping, Coreference Resolution, Triple Integration, and Predicate Mapping. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c0">As a general overview of the whole pipelined process, we first perform open relation extraction to obtain a set of binary relation tuples, one for each sentence in the input article, and then reduce the ambiguities (pronouns, different references to the same entity, etc.) in the tuples. After extracting all the entities and relations, identical entities are grouped using coreference chains from the Coreference Resolution component. Since entities in a given group might have various representations, a representative for the group of coreferring entities is then selected by a voting algorithm in the Triple Integration component. Empirically, the best voting scheme seems to just choose the majority excluding pronouns as the group representative. After that, all entities belonging to the group found in the relation triples are replaced by this representative, and we map it to either a new ID or an ID in the existing KG. Once we are done with entities, all the relations are vectorized using knowledge graph embedding and the pairwise cosine similarity among all the relation vectors calculated. We then collapse all the similar relations based on a predefined similarity threshold and output the resulting set of triples. Finally, the triples are converted to a specific JSON format to generate a knowledge graph.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 358.67px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 358.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c20"><span class="c3">Figure 1. Our pipelined process of generating knowledge graphs from text.</span></p><h3 class="c27" id="h.4yq64s75g063"><span class="c9">2.2 Entity Mapping </span></h3><p class="c5"><span class="c10">The aim of the Entity Mapping component is to map an entity extracted from unstructured text to an ID as output. If an extracted entity can be mapped to an identical entity in the knowledge graph, the ID of such an entity in the KG would be used as representative for the extracted entity. Otherwise, a new ID is assigned to the extracted entity. The expected results of this component are a set of mapping entities for each extracted entity. This mapping also facilitates other tasks down the pipeline, as will be shown later.</span></p><p class="c1 c12"><span class="c0"></span></p><h3 class="c27" id="h.biuqybju8g31"><span class="c9">2.3 Coreference Resolution </span></h3><p class="c1"><span class="c0">We then used Allennlp&rsquo;s source file to run coreference resolution extraction on the raw data text. This produced a cluster of nouns and pronouns that refer to the same entity. Each cluster was essentially an equivalence class that contained &nbsp;entities referring to the same thing. We for each equivalence class, we picked a &ldquo;most representative entity&rdquo; to represent the class by finding the most frequently used entity that wasn&rsquo;t a pronoun. An alternative way to find this most representative entity would have been to find the first mention of the entity that wasn&rsquo;t a pronoun. Both were valid options that produced similar results, but we went with the former method because it was simpler and more consistent. </span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 112.00px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 112.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c23"><span class="c10 c34">Figure 2. Example of Coreference Resolution Classification</span></p><h3 class="c2" id="h.obzq5psetisi"><span class="c0">2.4 Triple Extraction</span></h3><p class="c5"><span class="c10">The quality of the knowledge graph is largely dependent upon the extracted relation tuples, so we tried a couple of different existing Open Relation Extraction systems and also built our own system. We found that the Relation Extraction model from AllenNLP and our transformer model yielded the best results. </span></p><h4 class="c30" id="h.4abh9ljy07t7"><span class="c10 c26 c46">2.4.1 Triple Extraction using an existing open IE system (AllenNLP)</span></h4><h4 class="c35 c39" id="h.4abh9ljy07t7-1"><span class="c0">We used the OpenIE model from AllenNLP to find relationships in each sentence of the text, each of which contained one predicate and at least one entity. We then filtered by relationships that contained exactly two entities separated by a predicate since this related two entities by a direct relationship to generate our relation tuples.</span></h4><p class="c32 c12"><span class="c7"></span></p><p class="c5"><span class="c10">We also noticed that many extracted entities were not proper objects (nouns), which could not be incorporated into a knowledge graph. Therefore, we validated the extracted entities using a part-of-speech (POS) tagger and removed all tuples with entities starting with a verb/prep./etc.. We also truncated entities with long names. Examination over a large sample showed that this additional validation step was highly effective in filtering out malformed tuples.</span></p><p class="c32 c12"><span class="c7"></span></p><h4 class="c39 c35" id="h.4abh9ljy07t7-2"><span class="c0">2.4.2 Triple Extraction using a sequence-to-sequence model</span></h4><p class="c5"><span class="c0">Relation extraction is at the core of our pipelined process, yet none of the existing relation extraction models yields satisfying triples that can be directly used to generate a graph (without the need of any validation procedure). In particular, tuples generated by Stanford Open IE are overly abstract and miss many important details we might care about in the original sentence. AllenNLP Open IE keeps both a truncated and detailed extraction, but tend to mistakenly cast verbs or adjuncts as entities. Hence, we decided to develop our own relation extraction model and compare it with existing systems. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c0">Inspired by Neural Machine Translation, we formulate the open relation extraction problem as a sequence to sequence generation task. More specifically, given an input sentence, the task is to produce a sequence containing the subject, object, and predicate delimited by special tokens indicating the type of each span. For instance, given the input &ldquo;Barack Obama was the President of the United States.&rdquo; our objective is to generate the sequence &ldquo;&lt;arg1&gt;Barack Obama &lt;/arg1&gt; &lt;rel&gt;was&lt;/rel&gt; &lt;arg2&gt;President of the United States &lt;/arg2&gt;.</span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c0">Given the recent success of self-attention-based neural networks in a variety of sequence processing works, we apply the transformer architecture (Vaswani et al., 2017) to this problem. A transformer is composed of an encoder and a decoder, each of which contains several blocks. An encoder block contains a multihead attention layer followed by a fully connected feed-forward layer. The decoder block looks the same except the first multi-head attention layer is masked so that it can not peek ahead at future inputs, and it has an additional multi-head attention layer that gets some of its inputs from the corresponding encoder block. The input sentences are first tokenized using a byte-pair tokenizer from OpenAI&rsquo;s GPT model (Radford et al. 2018). The final output of the last decoder block is fed into a final linear layer with a softmax activation function to generate a probability distribution over the output vocabulary. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c0">In addition to using the tokenizer from OpenAI&rsquo;s GPT, we also used the pretrained token embeddings to warm start our models as these embeddings were the result of a significant amount of training on much larger corpora (so they might capture some low-level features of the language). The size of our source and target vocabulary is about 35000 (tokens). Because of memory limits, the training set was split into chunks and each chunk was run for 1-2 epochs. We used the Adam optimizer with a decaying learning rate schedule. We also applied dropout to linear layers with 0.1 drop probability. Finally, we employed label smoothing with epsilon = 0.1 to make the model more unsure, a technique that had been shown to improve accuracy. Our final model has dimension 768, feed-forward dimension 1200, 6 self-attention heads, and 5 encoder and decoder blocks. In total, the model has 123,506,387 parameters. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c10">We trained the model over 4 days on a single Nvidia K80 with 12 GB of vRAM. We had a batch size of 9000 tokens, and batches were sorted by sentence length to minimize the number of padding tokens to maximize efficiency. We used training data</span><sup class="c10"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">&nbsp;made available by (Cui et al., 2018), which was gathered from Wikipedia dump 20180101. The whole training set contains 36,247,584 (sentence, tuple) pairs in total. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c0">Figure 3. Precision-Recall Curve of extractions by our Transformer model and 5 other Open IE systems</span></p><p class="c5 c12"><span class="c11 c4"></span></p><p class="c5"><span class="c10">We also evaluated our model on an Open IE benchmark</span><sup class="c10"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c0">&nbsp;created by (Stanovsky et al., 2018), with 3200 sentences from Wikipedia and the Wall Street Journal that has 10,359 extractions. For each sentence, our extraction was lined up with multiple gold extractions and the extraction was considered valid if it achieved a high enough lexical coverage of the reference (defined by the percentage of words covered). Subsequently, the precision and recall of our system were analyzed on different confidence thresholds, and the area under the PR curve calculated. &nbsp;</span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c10">It is observed from the Precision-Recall curve (Figure 3) that our transformer model performs significantly better than 4 of the 5 existing rule-based Open IE systems and achieves comparable results with the current best systems OpenIE-4. None of our models achieves a recall as high as some other systems though (noticeably ClausIE, which is best at recall) because our model only produces one extraction per sentence. </span></p><p class="c5 c12"><span class="c0"></span></p><p class="c5"><span class="c0">2.5 Triple Integration</span></p><p class="c1"><span class="c10">The relation tuples produced decent results, but often contained many ambiguous entities, like pronouns. For example, we might have the following cluster of entities that refer to the same person: [&lsquo;Obama&rsquo;, &lsquo;he&rsquo;, &lsquo;Barack&rsquo; &lsquo;Barack Obama&rsquo;, &lsquo;Barack Obama&rsquo;, &lsquo;president of the United States&rsquo;]. We refer to these clusters as equivalence classes because all the entities are equivalent in terms of what they represent. To resolve these entities, as well as collapse nodes that really referred to the same thing when building the graph, we used the equivalence classes generated in the coreference clustering. For each entity, we replaced it with its </span><span class="c4">most representative entity (MRE) </span><span class="c0">from its respective equivalence class if it had one. We picked either the most frequent or first occurrence (excluding pronouns) entity as the most representative entity, and in practice we found that the first method worked slightly better.</span></p><p class="c1 c12"><span class="c0"></span></p><p class="c1"><span class="c10">Many entities were made up of nouns or pronouns that were found in multiple clusters. For these cases, we only replaced the entity if at least half the entities were in the same equivalence class. This helped resolve problems where a complex entity made up of multiple smaller entities was replaced by a single entity from the group. </span></p><h3 class="c2" id="h.6mvw026lfdq1"><span class="c10 c26">2.6 Predicate Mapping</span></h3><p class="c1"><span class="c10">To generate a knowledge graph, it was necessary to combine equivalent/similar relations. As an example, relations such as &ldquo;premiered&rdquo; and &ldquo;released&rdquo; as pertaining to a movie should be considered the same relation. To do this, we first mapped the relations to a vector space using the TransE knowledge graph embedding. TransE works by representing the entities (</span><span class="c4">h</span><span class="c10">, </span><span class="c4">t</span><span class="c10">) as vectors in a d-dimensional space with the relation vector (</span><span class="c4">r</span><span class="c10">) being a translation vector between the two. Mathematically, this can be described as optimizing the scoring function </span><span class="c10 c34">f</span><span class="c10 c16">r</span><span class="c10">&nbsp;(h, t) = </span><span class="c10">- || </span><span class="c4">h + r - t</span><span class="c10">&nbsp;||</span><span class="c10 c16">&frac12;</span><span class="c10">&nbsp;under the constraints || </span><span class="c4">h</span><span class="c10">&nbsp;||</span><span class="c10 c16">2</span><span class="c10">&nbsp;= || </span><span class="c4">t ||</span><span class="c10 c16">2</span><span class="c0">&nbsp;= 1. This was trained on the triples that were generated. Once the embeddings were completed, the relation vectors were combined if their cosine similarity score was above a certain threshold.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 393.50px; height: 393.50px;"><img alt="" src="images/image5.png" style="width: 393.50px; height: 393.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c12"><span class="c3"></span></p><p class="c5 c12"><span class="c3"></span></p><p class="c5"><span class="c10 c34">Figure 4. Heatmap demonstrating similarity between various relations</span></p><p class="c1"><span class="c11 c4">3 Results</span></p><h2 class="c17" id="h.623k4w2gkt4r"><span class="c0">3.1 Tuples </span></h2><p class="c1"><span class="c10">Our outputs are</span><span class="c0">&nbsp;open relation tuples of the form (subject, relation, object). &nbsp;Below are some examples.</span></p><p class="c1 c12"><span class="c0"></span></p><a id="t.e1e471538513d932f07e31d6a0a2eb0412a07300"></a><a id="t.0"></a><table class="c38"><tbody><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c11 c4">Input sentence</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c11 c4">Extraction by our transformer model</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">As a group , the team was enshrined into the Basketball Hall of Fame in 1959 .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">the team ||| was enshrined ||| into the basketball hall of fame in 1959 .</span></p></td></tr><tr class="c40"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">Certain fractional quantum Hall phases appear to have the right properties for building a topological quantum computer .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">certain fractional quantum hall phases ||| to have ||| the right properties for building a topological quantum computer .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">Ballast tanks are equipped to change a ship &rsquo;s trim and modify its stability .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">Ballast tanks ||| are equipped ||| to change a ship &rsquo;s trim and modify its stability .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">In Taiwan , the locals speak a version of the Minnan language which is called Taiwanese .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">the minnan language ||| is called ||| taiwanese .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">In 2004 it was expected that redevelopment work in the remaining subway would probably obliterate what remains exist .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">redevelopment work in the remaining subway ||| would probably obliterate ||| what remains exist .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">The town and surrounding villages were hit by two moderate earthquakes within ten years .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">the town and surrounding villages ||| were hit ||| by two moderate earthquakes within ten years .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">There were 22.2 % of families and 23.8 % of the population living below the poverty line , including 15.8 % of under eighteens and 37.5 % of those over 64 .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">22.2 % of families and 23.8 % of the population ||| living ||| below the poverty line .</span></p></td></tr><tr class="c19"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">These are known as Porter &rsquo;s three generic strategies and can be applied to any size or form of business .</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0">Porter &rsquo;s three generic strategies ||| can be applied ||| to any size or form of business .</span></p><p class="c8 c12"><span class="c0"></span></p></td></tr></tbody></table><p class="c1 c12"><span class="c0"></span></p><p class="c25"><span class="c3">Table 1. Sample open relation extractions by our transformer model</span></p><p class="c1"><span class="c0">We observed that our transformer is able to correctly identify the boundary of arguments and relations in most cases. Sometimes our transformer is even able to correctly replace an ambiguous pronoun in an argument with what it&#39;s referring to in the context, as in the last example above. But eventually, all the pronouns would be replaced during coreference resolution and triple integration.</span></p><p class="c1 c12"><span class="c0"></span></p><h3 class="c2" id="h.e9eeo7m5rjol"><span class="c9">3.2 Graph Visualization</span></h3><p class="c21"><span class="c0">We decided to use a force-directed graph to visualize relation triples and how they are laid out in a knowledge graph. The force layout is a class of graph layout algorithms in D3 that calculates the positions of each node by simulating an attractive force between each pair of linked nodes, as well as a repulsive force between the nodes. We assigned entities to each node and relations to the graph&rsquo;s links / edges. Typically, the attractive force acts like a spring between the nodes, calculated by Hooke&rsquo;s law. On the other hand, two nodes are pushed away from each other using Coulomb&rsquo;s law. It is a commonly implemented graph drawing algorithm because of its flexibility and intuitiveness, as it requires no special knowledge of graph theory. We also decided to use the force layout for its interactivity and customizability.</span></p><p class="c21"><span class="c0">In the beginning, we tried using other techniques for visualizing knowledge graphs. In particular, we experimented using the networkD3 package in R and ShinyR for creating interactive web apps and visualizations. However, after a few weeks of experimenting with various features, we decided that using D3 would allow us to customize our graph more freely.</span></p><p class="c21"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 320.00px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 320.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c21"><span class="c0">In general, this viz does a decent job at showing two things. First, it shows the relationships between entities and lays it out in an organized manner. Secondly, we added features that allow us to drag nodes, and be able to see individual relationships (highlighted in red) when hovering over them as well. This gives the viewer an engaging interactivity aspect. </span></p><p class="c21"><span class="c0">That being said, there are improvements that could be made to the visualization. A method of better aligning entities would reduce visual clutter, and panel of some sort could be added on the side that prints out the full relational sentences when hovering over a node or edge. </span></p><p class="c6"><span class="c0"></span></p><h2 class="c17" id="h.v0axiktnplr1"><span class="c11 c4">4 Future Goals</span></h2><p class="c21"><span class="c11 c4">Improving Recall (in relation extraction)</span></p><p class="c21"><span class="c10">Despite the superior accuracy of our transformer system, our main weakness is with recall. Because we can only make 1 extraction per sentence, our approach is inherently limited on a data set with over 10000 triples and 3200 sentences. One potential workaround of this limitation, keeping within our general approach, is to use something akin to a modified beam search decoding, basically a parallel hypothesis greedy search. The modification would be to start the search with an initial set of candidates that already contain the first token of candidate arguments. The search would then decode each candidate into its most likely triple. Each fully decoded candidate would be associated with a probability value as before. This value could be used as a threshold, and then we can take the top K candidates that achieve this threshold.</span></p><p class="c21"><span class="c11 c4">Downstream Application</span></p><p class="c21"><span class="c0">Our initial motivation for building a knowledge graph is to help with the task of question-answering. Our entity and predicate mapping processes greatly reduced the heterogeneity problem and increased searchability over a knowledge graph. As a result, we can see its potential in question answering and other query expansion tasks.</span></p><p class="c21"><span class="c11 c4">More Interactive Visualization</span></p><p class="c21"><span class="c0">As mentioned before, we would like to find a way to make the text and overall graph more organized. Additionally, we would like to implement more interactive features for the user to explore relations. </span></p><p class="c6"><span class="c0"></span></p><div><p class="c35 c12 c37"><span class="c7"></span></p></div><hr class="c33"><div><p class="c8 c35"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c18 c24">&nbsp;Github repository: https://github.com/CornellDataScience/Insights-Knowledge-Graphs</span></p></div><div><p class="c8 c35"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c24 c18">https://1drv.ms/u/s!ApPZx_TWwibImHl49ZBwxOU0ktHv</span></p><p class="c8 c35 c12"><span class="c24 c18"></span></p></div><div><p class="c8 c35"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c24 c18">&nbsp;https://github.com/gabrielStanovsky/oie-benchmark</span></p><p class="c8 c35 c12"><span class="c24 c18"></span></p></div></body></html>